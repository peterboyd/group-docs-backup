{"can_edit": true, "from": {"name": "Ljubomir Ljubojevi\u0107", "id": "100000182091980"}, "subject": "Software RAID on RHEL 6 ", "updated_time": "2013-07-08T19:36:15+0000", "can_delete": false, "created_time": "2012-11-12T10:37:57+0000", "message": "<p><strong>* NOTICE 1: Text is changed by Ljubomir Ljubojevic in some aspects. Original is on <a href=\"http://l.facebook.com/l.php?u=http%3A%2F%2Flinux.dell.com%2Ffiles%2Fwhitepapers%2FSoftware_RAID_on_Red_Hat_Enterprise_Linux_v6.pdf&amp;h=9AQE2lGAb&amp;s=1\" target=\"_blank\" rel=\"nofollow\" onmouseover=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/linux.dell.com\\/files\\/whitepapers\\/Software_RAID_on_Red_Hat_Enterprise_Linux_v6.pdf&quot;);\" onclick=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/l.facebook.com\\/l.php?u=http\\u00253A\\u00252F\\u00252Flinux.dell.com\\u00252Ffiles\\u00252Fwhitepapers\\u00252FSoftware_RAID_on_Red_Hat_Enterprise_Linux_v6.pdf&amp;h=9AQE2lGAb&amp;s=1&quot;);\">http://linux.dell.com/files/whitepapers/Software_RAID_on_Red_Hat_Enterprise_Linux_v6.pdf</a></strong></p><p><strong><br /></strong></p><p><strong>* NOTICE 2: /dev/sd? and /dev/md? names are deliberetly changed to avoid accidental damage of existing partitions, and force you to edit them before you run given command.</strong></p><p><strong></strong></p><p><em><strong><br /></strong></em></p><p><em><strong>Software RAID on RHEL 6</strong></em></p><p><br /></p><p>Installation, Migration and Recovery</p><p>November 2010</p><p>Ashokan Vellimalai</p><p>Raghavendra Biligiri</p><p>Dell \u2502 Enterprise Operating Systems</p><p>THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL ERRORS AND TECHNICAL INACCURACIES. THE CONTENT IS PROVIDED AS IS, WITHOUT EXPRESS OR IMPLIED WARRANTIES OF ANY KIND.</p><p>\u00a9 2010 Dell Inc. All rights reserved. Reproduction of this material in any manner whatsoever without the express written permission of Dell Inc. is strictly forbidden. For more information, contact Dell.</p><p>Dell, the DELL logo, and PowerEdge, are trademarks of Dell Inc. Red Hat Enterprise Linux\u00ae is a registered trademark of Red Hat, Inc. in the United States and/or other countries. Other trademarks and trade names may be used in this document to refer to either the entities claiming the marks and names or their products. Dell Inc. disclaims any proprietary interest in trademarks and trade names other than its own.</p><p>November 2010</p><p><br /></p><p>Contents</p><p>Introduction ................................................................... 4</p><p>Setting up Software RAID in RHEL 6 .................................... 4</p><p>Setup during Installation .................................................... 4</p><p>Setup after Installation ...................................................... 5</p><p>Migration of Storage from Non-RAID to RAID Configurations ..... 6</p><p>Resizing an existing RAID Partition ....................................... 8</p><p>Recovery from a Broken RAID ........................................... 11</p><p>Automatic Failover ........................................................... 11</p><p>Manual Failover ............................................................... 11</p><p>Adding a Spare Disk to the Array ......................................... 12</p><p>References ..................................................................... 13</p><p><br /></p><p>Introduction</p><p>Software RAID is RAID that is implemented at the software layer without the need for a dedicated hardware RAID controller on the system. Software RAID can be created on any storage block device independent of storage controllers. On Linux based operating system (OS), software RAID functionality is provided with the help of the md(4) (Multiple Device) driver and managed by the mdadm(8) utility. \u201cmd\u201d and \u201cmdadm\u201d in RHEL 6 support RAID levels 0, 1, 4, 5, 6, and 10.</p><p><br /></p><p>Some notable advantages in using Software RAID over hardware RAID are:</p><p>\u2022 Software RAID is controller Independent which makes it a cost-effective solution.</p><p>\u2022 The RAID solution can easily be migrated to any storage block device.</p><p>\u2022 The entire software stack runs on a host CPU, with modern multi-core CPUs, this ensures efficient CPU utilization.</p><p>\u2022 Software RAID provides a level of abstraction on underlying storage devices/technologies</p><p><br /></p><p>This document attempts to provide step-by-step procedures that can be followed to:</p><p>\u2022 Set up software RAID in RHEL 6</p><p>\u2022 Migrate existing storage from Non-RAID to Software RAID</p><p>\u2022 Resize RAID volumes</p><p>\u2022 Recover from a broken RAID</p><p>This document uses RAID-1 as an example while working with Software RAID. The procedure can however be applied to other RAID types as applicable. Please consult the mdadm(8) man page for details on exact options for various RAID types.</p><p><br /></p><p>We used a Dell PowerEdgeTM R510 server with a Dell PERC H200 storage controller on the system for this procedure. All the storage volumes were exported directly to the OS without using any controller hardware RAID features.</p><p><br /></p><p>Note: Ensure that all data backed up before performing any of these procedures</p><p><br /></p><p>Setting up Software RAID in RHEL 6</p><p>Setup during Installation</p><p>The RHEL 6 installer (anaconda) has functionality that enables the OS to be installed on a software RAID partition. This section describes the steps to install RHEL 6 on a RAID-1 partition.</p><p>1. Start the RHEL 6 installer and follow the on-screen installation instructions and select the \u201cCustom\u201d layout for installation.</p><p>2. Create a minimum of two partitions to create RAID-1 device type and set the File System Type as software raid.</p><p>3. Create a RAID-1 device from RAID members created in Step2, select the filesystem and RAID-1 level</p><p>Here is the minimum number of software RAID partitions required for each RAID level:</p><p>\u2022 RAID 0,1,10 \u2013 2 partitions</p><p>\u2022 RAID 4,5 \u2013 3 partitions</p><p>\u2022 RAID 6 \u2013 4 partitions</p><p>4. After creating all the necessary partitions (/boot, /, swap, etc.) on RAID-1 volume, proceed with the installation.</p><p>5. Once the installation is completed, the OS will boot successfully from the partitions on the RAID volume.</p><p>Note: Ensure that the boot-loader is installed on the first disk and not on the RAID device.</p><p>Installing boot-loader on the RAID device may result in failure to boot the OS after installation.</p><p><br /></p><p>Setup after Installation</p><p>Software RAID volumes can be created on a running system post install as well. Ensure that the partition/s on which the OS is installed are not used for creating software RAID partitions, failure to do that may results in re-installing OS on the system.</p><p>Following section describes steps to create RAID-1 partition on the system.</p><p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1 and /dev/sdl1. sdj1 and sdl1 are un-used partitions on this system <br /></p><p><br /></p><p>Note: &quot;--metadata=0.90&quot; is only used when you create /boot partition, because CentOS 6.x (6.4) still does not support boot from default metadata 1.2</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90</p><p>mdadm: array /dev/md9 started.</p><p>2. Create ext4 filesystem layout on the md device and add array details mdadm \u2013-detail \u2013scan to /etc/mdadm.conf file. Mount the device /dev/md9 on the system to use it.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 /data/</p><p>3. Add a new entry in /etc/fstab file to auto mount the md raid partition, whenever system reboots.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p><p>/dev/md9 /data ext4 defaults 1 1</p><p><br /></p><p>Migration of Storage from Non-RAID to RAID Configurations <br /></p><p>It is possible to migrate to software raid, the \u201c/\u201d partition without having to re-install the operating system if you installed RHEL 6 OS without software raid volumes. This section explains how migration of storage from non-raid to raid configuration can be achieved.</p><p><br /></p><p>At a high level, here is how it can be achieved:</p><p>\u2022 Prepare the new storage volume</p><p>\u2022 Update fstab and grub configuration to boot from newly created storage volume</p><p>\u2022 Sync the data from the old partitions to the new storage volume</p><p>\u2022 Install the boot loader on new storage volume</p><p>\u2022 Add the old partition volume to the md raid-1 set</p><p> </p><p><br /></p><p>Prepare the new storage volume</p><p>1. Create the partition layout on the /dev/sdj volume similar to /dev/sdh.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -d /dev/sdh | sfdisk --force /dev/sdj</p><p>Device\u00a0\u00a0\u00a0\u00a0 Boot\u00a0\u00a0\u00a0\u00a0\u00a0 Start\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 End\u00a0\u00a0 #sectors\u00a0 Id System</p><p>/dev/sdj1\u00a0 *\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2048\u00a0 20482047\u00a0 20480000 83 Linux</p><p>/dev/sdj2\u00a0\u00a0\u00a0\u00a0\u00a0 20482048\u00a0 24578047\u00a0\u00a0\u00a0 4096000 82 Linux swap / Solaris</p><p>Warning: partition 1 does not end at a cylinder boundary</p><p>Successfully wrote the new partition table</p><p><br /></p><p>2. Set the partition id of /dev/sdj1 to Linux RAID</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdj 1 fd</p><p>Done</p><p><br /></p><p>Create RAID-1 using mdadm utility:</p><p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1. Mark the first volume as \u201cmissing\u201d, which will be sdh volume, and it will be added later in the steps. Since sdh has OS installed, we have to add this to raid array after copying the contents from sdj to sdh drive.</p><p><br /></p><p>Note: &quot;--metadata=0.90&quot; is only used when you create /boot partition, because CentOS 6.x (6.4) still does not support boot from default metadata 1.2</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 missing /dev/sdj1 --metadata=0.90</p><p>mdadm: array /dev/md9 started.</p><p>2. Create ext4 filesystem layout on the md device and add the array details mdadm \u2013-detail \u2013scan to /etc/mdadm.conf file.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p><p>Update fstab and grub configuration to boot from newly created storage volume:</p><p>1. Modify the /etc/fstab and /boot/grub/menu.lst with md device.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# blkid | grep \u2013i md9 <br /></p><p>/dev/md9: UUID=&quot;016db049-6802-4369-bf66-bd48aad15395&quot; TYPE=&quot;ext4&quot;</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p><p>#UUID=38b56dff-c6d2-434f-bb48-25efb97f3a58 / 1 1 ext4 defaults</p><p>UUID=016db049-6802-4369-bf66-bd48aad15395 / 1 1 ext4 defaults</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/menu.lst</p><p>2. Add sdj to the device map entry to install the grub on the sdj device.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p><p># this device map was generated by anaconda</p><p>(hd0) /dev/sdh</p><p>(hd1) /dev/sdj</p><p><br /></p><p>Sync the data from the old partitions to the new storage volume:</p><p>1. Since we are trying to replicate the contents from currently running partition. It is recommended that you execute the following steps in run level1</p><p>2. Mount the array volume and copy the contents from sdh1 to md9.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 //</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# rsync -aqxP / //</p><p><br /></p><p>Install the boot loader on new storage volume:</p><p>1. Install the boot loader on the sdj device.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdj</p><p>Installation finished. No error reported.</p><p>This is the contents of the device map /boot/grub/device.map.</p><p>Check if this is correct or not. If any of the lines is incorrect, fix it and re-run the script `grub-install&#039;.</p><p># this device map was generated by anaconda</p><p>(hd0) /dev/sdh</p><p>(hd1) /dev/sdj</p><p><br /></p><p>2. Reboot the system and verify the system has booted with the md device using the mount command.</p><p><br /></p><p>Add the old partition volume to the md raid-1 set:</p><p>1. Change the partition of sdh1 and add the sdh1 device to md9 array and allow the resync to complete from sdh1 to sdj1.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdh 1 fd</p><p>Done</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdh1</p><p>mdadm: added /dev/sdh1</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# watch cat /proc/mdstat</p><p>2. Reboot the system to verify the md migration completed successfully.</p><p>3. Run the cat /proc /mdstat command to check the status of the running array.</p><p><br /></p><p>Resizing an existing RAID Partition</p><p>The Linux software-RAID solution allows us to resize (increase or decrease) the RAID partition size. Following steps explain how to increase the size of existing software RAID partition (data and OS partitions).</p><p><br /></p><p>Here is what is required:</p><p>\u2022 Prepare partitions of the new size desired</p><p>\u2022 Replace both RAID members with newly created partitions by breaking existing RAID</p><p>\u2022 Resize the RAID array</p><p>\u2022 If dealing with OS partitions, Prepare the new RAID volume to be bootable</p><p><br /></p><p>Prepare partitions of the new size desired:</p><p>1. Initially create the md9 raid-1 level with a size approximately 100 GB comprised of both the sdh1 and sdj1 volumes. The md-0 array RAID set will be increased to approximately 200 GB by using the sdl1 and sdm1 volumes.</p><p>2. Create the new RAID partition of increased size on sdl and sdm. In this example, we created a new RAID partition of approximately 200GB in size on the sdl volume.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdl 1 fd</p><p>Done</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# fdisk -l /dev/sdl</p><p>Disk /dev/sdl: 1000.2 GB, 1000204886016 bytes</p><p>255 heads, 63 sectors/track, 121601 cylinders</p><p>Units = cylinders of 16065 * 512 = 8225280 bytes</p><p>Sector size (logical/physical): 512 bytes / 512 bytes</p><p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p><p>Disk identifier: 0x4c0a9054</p><p>Device\u00a0\u00a0\u00a0\u00a0 Boot Start\u00a0\u00a0\u00a0\u00a0 End\u00a0\u00a0\u00a0\u00a0\u00a0 Blocks\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Id System</p><p>/dev/sdl1 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0\u00a0 25000\u00a0 200812468+ fd Linux raid autodetect</p><p><br /></p><p>Replace both RAID members with newly created partitions by breaking existing RAID:</p><p>1. Set the sdj1 volume to faulty and remove the volume from the RAID set.</p><p><br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm /dev/md9 \u2013-fail /dev/sdj1 \u2013remove /dev/sdj1<br />mdadm: set /dev/sdj1 faulty in /dev/md9<br />mdadm: hot removed /dev/sdj1 from /dev/md9<br /><br />2. Add the new partition to the RAID set and allow the resync to complete on the new partition added to the RAID set. Run cat /proc/mdstat to show the status of resynchronization.<br /></p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p><p>mdadm: added /dev/sdl1</p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p><p>Personalities : [RAID-1]</p><p>md9 : active RAID-1 sdl1[2] sdh1[0]</p><p>102399928 blocks super 1.0 [2/2] [UU]</p><p>bitmap: 1/1 pages [4KB], 65536KB chunk</p><p>unused devices: </p><p><br /></p><p>3. Repeat the above steps adding the /dev/sdm1 partition. Remove the /dev/sdh1 partition from the RAID set. Allow the resynchronization to complete on the /dev/sdm1 partition.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p><p>Personalities : [RAID-1]</p><p>md9 : active RAID-1 sdm1[3] sdl1[2]</p><p>102399928 blocks super 1.0 [2/1] [_U]</p><p>[&gt;....................]</p><p>recovery =</p><p>finish=20.7min speed=81894K/sec</p><p>bitmap: 1/1 pages [4KB], 65536KB chunk</p><p>0.3%</p><p>(409472/102399928)</p><p>unused devices: </p><p><br /></p><p>Resize the RAID array:</p><p>1. Set the /dev/md9 partition size to use the new volume partition size and allow the resync to complete.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p><p>mdadm: Cann[ot set device size for /dev/md9: Device or resource busy</p><p>Bitmap must be removed before size can be changed</p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --bitmap none</p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p><p>mdadm: component size of /dev/md9 has been set to 200812396K</p><p><br /></p><p>2. Resize the file system of the /dev/md9 partition to increase the file system size. Now the df \u2013h command shows the increased md RAID set.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# resize2fs /dev/md9</p><p>resize2fs 1.41.12 (17-May-2010)</p><p>Filesystem at /dev/md9 is mounted on /; on-line resizing required old desc_blocks = 7, new_desc_blocks = 12</p><p>Performing an on-line resize of /dev/md9 to 50203099 (4k) blocks.</p><p>The filesystem on /dev/md9 is now 50203099 blocks long.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# df -h</p><p>Filesystem\u00a0 Size Used Avail Use% Mounted on</p><p>/dev/md9 189G 1.4G 178G\u00a0\u00a0\u00a0\u00a0 1% /</p><p>tmpfs\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.9G\u00a0\u00a0\u00a0\u00a0 0 1.9G\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0% /dev/shm</p><p><br /></p><p>If dealing with OS partitions, prepare the new RAID volume to be bootable:</p><p><br /></p><p>Note: Following steps are required to boot the Linux system, if you are replacing the drive from RAID array which has boot loader and file system already installed.</p><p>Before resizing the RAID partition, ensure that the md-raid set has two active drives, sdh1 and sdj1 and that grub is installed in /dev/sdh. After the resizing operation, the md-raid set has been replaced with sdl1 and sdm1 volumes and boot loader has been installed on sdl volume.</p><p><br /></p><p>1. Add the sdl and sdm entries in device.map file.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p><p># this device map was generated by anaconda</p><p>(hd0) /dev/sdh</p><p>(hd1) /dev/sdj</p><p>(hd2) /dev/sdl</p><p>(hd3) /dev/sdm</p><p><br /></p><p>2. Install the grub boot loader on the /dev/sdl volume and then remove the older drives from the system.</p><p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdl</p><p>Installation finished. No error reported.</p><p>This is the contents of the device map /boot/grub/device.map. Check if this is correct or not. If any of the lines is incorrect, fix it and re-run the script `grub-install&#039;.</p><p># this device map was generated by anaconda</p><p>(hd0) /dev/sdh</p><p>(hd1) /dev/sdj</p><p>(hd2) /dev/sdl</p><p>(hd3) /dev/sdm</p><p><br /></p><p>3. Reboot the system and enter the storage controller BIOS configuration.</p><p>4. Change the boot drive to new drive where the boot loader is now installed.</p><p>5. Save the configuration and restart the system to boot from new RAID set partition.</p><p><br /></p><p>Recovery from a Broken RAID</p><p>In case of RAID failures on a system running Linux Software RAID (md) solution (for example, media failure or disk driver failure), it is possible to recover the system by any of these methods</p><p>\u2022 replacing the faulty disk</p><p>\u2022 adding a new disk</p><p>\u2022 using the spare disk</p><p><br /></p><p>In Linux software raid, recovery is achieved through \u201cfailover\u201d mechanisms. Failover mechanism ensures data protection by providing additional drives (spares) and can be automatic or manual.</p><p><br /></p><p>Automatic Failover</p><p>Linux md-raid solution has an intelligent monitor mechanism to detect hardware failure in RAID arrays.</p><p>If any disk in the RAID array fails, the monitors sets the failed drive to faulty and starts using one of the available spare drives for regeneration. To check the status of the RAID array, look at\u00a0 /proc/mdstat file.</p><p>To replicate a raid failure to check how automatic failover scenario works, follow the steps.</p><p>1. Create a RAID-1 setup with three raid partition. Minimum two raid partitions are required to create RAID-1 device and third raid partition is used as a spare disk, will be used as a replacement if one of the active RAID partition fails.</p><p>2. Simply pull out one of the disks which are active in the raid array and check the status of the array using the cat /proc/mdstat command, will show the removed drive as faulty and use the spare drive as replacement for date re-generation.</p><p><br /></p><p>Manual Failover</p><p>Faulty drives in the raid array can be replaced manually. Following steps discuss how to manually replace the faulty drive from the RAID-1 array:</p><p>1. Raid-1 setup is created with sdh1 and sdj1 partitions. Set the sdh1 drive to faulty.</p><p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -f /dev/md9 /dev/sdh1</p><p>mdadm: set /dev/sdh1 faulty in /dev/md9</p><p>2. Remove the faulty drive sdh1 from the array.</p><p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -r /dev/md9 /dev/sdh1</p><p>mdadm: hot removed /dev/sdh1 from /dev/md9</p><p>3. Replace the faulty drive with adding a new one to the array.</p><p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p><p>Device Added</p><p>4. Look at status of the RAID array by executing the cat /proc/mdstat command, showing /dev/sdl1 added to the RAID array. Also check that the resynchronization is complete.</p><p>[root&#064;DELL-PowerEdge-R510 ~]# cat /proc/mdstat</p><p>Personalities : [RAID-1]</p><p>md9 : active RAID-1 sdl1[2] sdj1[0]</p><p>40958908 blocks super 1.1 [2/2] [UU]</p><p>bitmap: 1/1 pages [4KB], 65536KB chunk</p><p><br /></p><p>Adding a Spare Disk to the Array</p><p>Spare disks provide additional protection to a raid configuration. If a disk fails in a raid array, the spare disk automatically replaces the failed drive; also the raid can be rebuilt automatically in the background. Spare drives can be added to the RAID array during the time of creation of the array or later.</p><p><br /></p><p>Adding spare disk during raid array creation</p><p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90 \u2013-spare-devices=1 /dev/sdm1</p><p>mdadm: array /dev/md9 started.</p><p><br /></p><p>Adding spare disk to an existing array.</p><p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p><p>mdadm: added /dev/sdl1</p><p>[root&#064;DELL-Poweredge-R510 ~]# cat /proc/mdstat</p><p>Personalities : [RAID-1]</p><p>md8 : active RAID-1 sdl1[2](S) sdh1[0] sdj1[1] sdl</p><p><br /></p><p>References</p><p>\u2022 <a href=\"http://l.facebook.com/l.php?u=http%3A%2F%2Fdocs.redhat.com%2Fdocs%2Fen-US%2FRed_Hat_Enterprise_Linux%2Findex.html&amp;h=hAQEGYfVo&amp;s=1\" target=\"_blank\" rel=\"nofollow\" onmouseover=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/docs.redhat.com\\/docs\\/en-US\\/Red_Hat_Enterprise_Linux\\/index.html&quot;);\" onclick=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/l.facebook.com\\/l.php?u=http\\u00253A\\u00252F\\u00252Fdocs.redhat.com\\u00252Fdocs\\u00252Fen-US\\u00252FRed_Hat_Enterprise_Linux\\u00252Findex.html&amp;h=hAQEGYfVo&amp;s=1&quot;);\">http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/index.html</a></p><p>\u2022 <a href=\"http://l.facebook.com/l.php?u=http%3A%2F%2Fwww.spinics.net%2Flists%2Fraid%2F&amp;h=pAQFx56O2&amp;s=1\" target=\"_blank\" rel=\"nofollow\" onmouseover=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/www.spinics.net\\/lists\\/raid\\/&quot;);\" onclick=\"LinkshimAsyncLink.swap(this, &quot;http:\\/\\/l.facebook.com\\/l.php?u=http\\u00253A\\u00252F\\u00252Fwww.spinics.net\\u00252Flists\\u00252Fraid\\u00252F&amp;h=pAQFx56O2&amp;s=1&quot;);\">http://www.spinics.net/lists/raid/</a></p><p>\u2022 <a href=\"https://www.facebook.com/l.php?u=https%3A%2F%2Fraid.wiki.kernel.org%2Findex.php%2FLinux_Raid&amp;h=DAQF6ICwI&amp;s=1\" target=\"_blank\" rel=\"nofollow\" onmouseover=\"LinkshimAsyncLink.swap(this, &quot;https:\\/\\/raid.wiki.kernel.org\\/index.php\\/Linux_Raid&quot;);\" onclick=\"LinkshimAsyncLink.swap(this, &quot;https:\\/\\/www.facebook.com\\/l.php?u=https\\u00253A\\u00252F\\u00252Fraid.wiki.kernel.org\\u00252Findex.php\\u00252FLinux_Raid&amp;h=DAQF6ICwI&amp;s=1&quot;);\">https://raid.wiki.kernel.org/index.php/Linux_Raid</a></p>-------------------------------------------------------------------------------------------<p><br /></p><p><br /></p><p>Stopping resync-a:</p><p>echo &quot;idle&quot; &gt; /sys/block/md0/md/sync_action</p><p><br /></p><p>Rename:</p><p><br /></p><p>Last time I did this it was from a later distribution --- either a RHEL-4 or FC-5 rescue CD, I cannot recall which at this point.\u00a0 I did it precisely for the reason you give, to move an existing /dev/md0 to another RHEL-4 box.\u00a0 But it wasn&#039;t hard, using mdadm.\u00a0 Knowing the devices belonging to the array (sda5 and sdb5 in this case), I just did:</p><p><br /></p><p># mdadm --stop /dev/md0</p><p># mdadm -A /dev/md6 -m0 --update=super-minor /dev/sda5 /dev/sdb5</p><p><br /></p><p>which stops the array as /dev/md0 and then reassembles it as /dev/md6.</p><p>The reassembly looks for devices which have an existing minor number of 0, not 6 (-m0), and then updates the minors in the superblocks to the new number.</p><p><br /></p><p>I believe the same options are present in taroon&#039;s mdadm.</p><p><br /></p><p>mdadm ---create --level=1 --raid-devices=2 /dev/sdh1 /dev/sdg1</p><p><br /></p><p>(Repair filesystem) Writing not allowed. Solution:</p><p>mount -w -o remount /</p><p><br /></p><p>Linux console scroll : Shift + PgDn ili Shift + PgUp</p><p><br /></p><p>Creating Boot RAID1:</p><p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdd1 /dev/sde1 --metadata=0.90</p><p><br /></p><p>Creating RAID10 far,2:</p><p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=10 --layout=f2 --raid-disks=2 /dev/sdd9 /dev/sde9</p><p><br /></p><p>6.3. Activate RAID Devices</p><p>The installation media doesn&#039;t include a default mdadm.conf file, so it must be created. An easy method is...</p><p><br /></p><p>mdadm --examine --scan &gt; /etc/mdadm.conf</p><p><br /></p><p>Next, activate all discovered RAID devices with...</p><p><br /></p><p>mdadm --assemble --scan</p><p><br /></p><p>cat /proc/sys/dev/raid/speed_limit_max</p><p>200000</p><p>cat /proc/sys/dev/raid/speed_limit_min</p><p>1000</p><p><br /></p><p>50MB/s:</p><p>echo 50000 &gt;/proc/sys/dev/raid/speed_limit_min</p>", "revision": "10151713930987728", "id": "10151254589767728", "icon": "https://fbstatic-a.akamaihd.net/rsrc.php/v2/y7/r/8zhNI-VGpiI.png"}