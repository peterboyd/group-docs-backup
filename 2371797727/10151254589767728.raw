{"can_edit": true, "from": {"name": "Ljubomir Ljubojevic", "id": "100000182091980"}, "revision": "10151254590247728", "updated_time": "2012-11-12T10:38:39+0000", "created_time": "2012-11-12T10:37:57+0000", "message": "<p><strong>* NOTICE 1: Text is changed by Ljubomir Ljubojevic in some aspects.</strong></p>\n<p><strong>* NOTICE 2: /dev/sd? and /dev/md? names are deliberetly changed to avoid accidental damage of existing partitions, and force you to edit them before you run given command.</strong></p>\n<p><strong></strong></p>\n<p><em><strong>Software RAID on RHEL 6</strong></em></p>\n<p>\u00a0</p>\n<p>Installation, Migration and Recovery</p>\n<p>November 2010</p>\n<p>Ashokan Vellimalai</p>\n<p>Raghavendra Biligiri</p>\n<p>Dell \u2502 Enterprise Operating Systems</p>\n<p>THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL</p>\n<p>ERRORS AND TECHNICAL INACCURACIES. THE CONTENT IS PROVIDED AS IS, WITHOUT EXPRESS OR</p>\n<p>IMPLIED WARRANTIES OF ANY KIND.</p>\n<p>\u00a9 2010 Dell Inc. All rights reserved. Reproduction of this material in any manner whatsoever without</p>\n<p>the express written permission of Dell Inc. is strictly forbidden. For more information, contact Dell.</p>\n<p>Dell, the DELL logo, and PowerEdge, are trademarks of Dell Inc. Red Hat Enterprise Linux\u00ae is a</p>\n<p>registered trademark of Red Hat, Inc. in the United States and/or other countries. Other trademarks</p>\n<p>and trade names may be used in this document to refer to either the entities claiming the marks and</p>\n<p>names or their products. Dell Inc. disclaims any proprietary interest in trademarks and trade names</p>\n<p>other than its own.</p>\n<p>November 2010</p>\n<p>Contents</p>\n<p>Introduction ............................................................................... 4</p>\n<p>Setting up Software RAID in RHEL 6 .......................................... 4</p>\n<p>Setup during Installation ........................................................... 4</p>\n<p>Setup after Installation .............................................................. 5</p>\n<p>Migration of Storage from Non-RAID to RAID Configurations ..... 6</p>\n<p>Resizing an existing RAID Partition ............................................. 8</p>\n<p>Recovery from a Broken RAID .................................................. 11</p>\n<p>Automatic Failover ................................................................... 11</p>\n<p>Manual Failover ........................................................................ 11</p>\n<p>Adding a Spare Disk to the Array ............................................. 12</p>\n<p>References ............................................................................... 13</p>\n<p>\u00a0</p>\n<p>Introduction</p>\n<p>Software RAID is RAID that is implemented at the software layer without the need for a dedicated</p>\n<p>hardware RAID controller on the system. Software RAID can be created on any storage block device</p>\n<p>independent of storage controllers. On Linux based operating system (OS), software RAID functionality</p>\n<p>is provided with the help of the md(4) (Multiple Device) driver and managed by the mdadm(8) utility.</p>\n<p>\u201cmd\u201d and \u201cmdadm\u201d in RHEL 6 support RAID levels 0, 1, 4, 5, 6, and 10.</p>\n<p>Some notable advantages in using Software RAID over hardware RAID are:</p>\n<p>\u2022 Software RAID is controller Independent which makes it a cost-effective solution.</p>\n<p>\u2022 The RAID solution can easily be migrated to any storage block device.</p>\n<p>\u2022 The entire software stack runs on a host CPU, with modern multi-core CPUs, this ensures</p>\n<p>efficient CPU utilization.</p>\n<p>\u2022 Software RAID provides a level of abstraction on underlying storage devices/technologies</p>\n<p>This document attempts to provide step-by-step procedures that can be followed to:</p>\n<p>\u2022 Set up software RAID in RHEL 6</p>\n<p>\u2022 Migrate existing storage from Non-RAID to Software RAID</p>\n<p>\u2022 Resize RAID volumes</p>\n<p>\u2022 Recover from a broken RAID</p>\n<p>This document uses RAID-1 as an example while working with Software RAID. The procedure can</p>\n<p>however be applied to other RAID types as applicable. Please consult the mdadm(8) man page for</p>\n<p>details on exact options for various RAID types.</p>\n<p>We used a Dell PowerEdgeTM R510 server with a Dell PERC H200 storage controller on the system for this</p>\n<p>procedure. All the storage volumes were exported directly to the OS without using any controller</p>\n<p>hardware RAID features.</p>\n<p>Note: Ensure that all data backed up before performing any of these procedures</p>\n<p>\u00a0</p>\n<p>Setting up Software RAID in RHEL 6</p>\n<p>Setup during Installation</p>\n<p>The RHEL 6 installer (anaconda) has functionality that enables the OS to be installed on a software</p>\n<p>RAID partition. This section describes the steps to install RHEL 6 on a RAID-1 partition.</p>\n<p>1. Start the RHEL 6 installer and follow the on-screen installation instructions and select the</p>\n<p>\u201cCustom\u201d layout for installation.</p>\n<p>2. Create a minimum of two partitions to create RAID-1 device type and set the File System Type as</p>\n<p>software raid.</p>\n<p>3. Create a RAID-1 device from RAID members created in Step2, select the filesystem and RAID-1</p>\n<p>level</p>\n<p>Here is the minimum number of software RAID partitions required for each RAID level:</p>\n<p>\u2022 RAID 0,1,10</p>\n<p>\u2013 2 partitions</p>\n<p>\u2022 RAID 4,5</p>\n<p>\u2013 3 partitions</p>\n<p>\u2022 RAID 6</p>\n<p>\u2013 4 partitions</p>\n<p>4. After creating all the necessary partitions (/boot, /, swap, etc.) on RAID-1 volume, proceed with</p>\n<p>the installation.</p>\n<p>5. Once the installation is completed, the OS will boot successfully from the partitions on the RAID</p>\n<p>volume.</p>\n<p>Note: Ensure that the boot-loader is installed on the first disk and not on the RAID device.</p>\n<p>Installing boot-loader on the RAID device may result in failure to boot the OS after installation.</p>\n<p>\u00a0</p>\n<p>Setup after Installation</p>\n<p>Software RAID volumes can be created on a running system post install as well. Ensure that the</p>\n<p>partition/s on which the OS is installed are not used for creating software RAID partitions, failure to do</p>\n<p>that may results in re-installing OS on the system.</p>\n<p>Following section describes steps to create RAID-1 partition on the system.</p>\n<p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1 and /dev/sdl1.</p>\n<p>sdj1 and sdl1 are un-used partitions on this system</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90</p>\n<p>mdadm: array /dev/md9 started.</p>\n<p>2. Create ext4 filesystem layout on the md device and add array details mdadm \u2013-detail</p>\n<p>\u2013scan to /etc/mdadm.conf file. Mount the device /dev/md9 on the system to use it.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 /data/</p>\n<p>3. Add a new entry in /etc/fstab file to auto mount the md raid partition, whenever system</p>\n<p>reboots.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p>\n<p>/dev/md9</p>\n<p>/data</p>\n<p>ext3</p>\n<p>defaults</p>\n<p>1 1</p>\n<p>\u00a0</p>\n<p>Migration of Storage from Non-RAID to RAID Configurations</p>\n<p>It is possible to migrate to software raid, the \u201c/\u201d partition without having to re-install the operating</p>\n<p>system if you installed RHEL 6 OS without software raid volumes. This section explains how migration of</p>\n<p>storage from non-raid to raid configuration can be achieved.</p>\n<p>At a high level, here is how it can be achieved:</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>Prepare the new storage volume</p>\n<p>Update fstab and grub configuration to boot from newly created storage volume</p>\n<p>Sync the data from the old partitions to the new storage volume</p>\n<p>Install the boot loader on new storage volume</p>\n<p>Add the old partition volume to the md raid-1 set</p>\n<p>Prepare the new storage volume</p>\n<p>1. Create the partition layout on the /dev/sdj volume similar to /dev/sdh.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -d /dev/sdh | sfdisk --force /dev/sdj</p>\n<p>Device Boot</p>\n<p>Start</p>\n<p>End</p>\n<p>#sectors Id System</p>\n<p>/dev/sdj1</p>\n<p>*</p>\n<p>2048 20482047</p>\n<p>20480000 83 Linux</p>\n<p>/dev/sdj2</p>\n<p>20482048 24578047</p>\n<p>4096000 82 Linux swap / Solaris</p>\n<p>Warning: partition 1 does not end at a cylinder boundary</p>\n<p>Successfully wrote the new partition table</p>\n<p>2. Set the partition id of /dev/sdj1 to Linux RAID</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdj 1 fd</p>\n<p>Done</p>\n<p>\u00a0</p>\n<p>Create RAID-1 using mdadm utility:</p>\n<p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1. Mark the first</p>\n<p>volume as \u201cmissing\u201d, which will be sdh volume, and it will be added later in the steps.</p>\n<p>Since sdh has OS installed, we have to add this to raid array after copying the contents</p>\n<p>from sdj to sdh drive.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 missing /dev/sdj1 --metadata=0.90</p>\n<p>mdadm: array /dev/md9 started.</p>\n<p>2. Create ext4 filesystem layout on the md device and add the array details mdadm \u2013-</p>\n<p>detail \u2013scan to /etc/mdadm.conf file.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p>\n<p>Update fstab and grub configuration to boot from newly created storage volume:</p>\n<p>1. Modify the /etc/fstab and /boot/grub/menu.lst with md device.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# blkid | grep \u2013i md9</p>\n<p>/dev/md9: UUID=&quot;016db049-6802-4369-bf66-bd48aad15395&quot; TYPE=&quot;ext4&quot;</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p>\n<p>#UUID=38b56dff-c6d2-434f-bb48-25efb97f3a58 /</p>\n<p>1 1</p>\n<p>UUID=016db049-6802-4369-bf66-bd48aad15395 /</p>\n<p>1 1</p>\n<p>ext4 defaults</p>\n<p>ext4 defaults</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/menu.lst</p>\n<p>2. Add sdj to the device map entry to install the grub on the sdj device.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p>\n<p># this device map was generated by anaconda</p>\n<p>(hd0)</p>\n<p>/dev/sdh</p>\n<p>(hd1)</p>\n<p>/dev/sdj</p>\n<p>Sync the data from the old partitions to the new storage volume:</p>\n<p>1. Since we are trying to replicate the contents from currently running partition. It is</p>\n<p>recommended that you execute the following steps in run level1</p>\n<p>2. Mount the array volume and copy the contents from sdh1 to md9.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 /mnt/</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# rsync -aqxP / /mnt/</p>\n<p>Install the boot loader on new storage volume:</p>\n<p>1. Install the boot loader on the sdj device.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdj</p>\n<p>Installation finished. No error reported.</p>\n<p>This is the contents of the device map /boot/grub/device.map.</p>\n<p>Check if this is correct or not. If any of the lines is incorrect,</p>\n<p>fix it and re-run the script `grub-install&#039;.</p>\n<p># this device map was generated by anaconda</p>\n<p>(hd0)</p>\n<p>/dev/sdh</p>\n<p>(hd1)</p>\n<p>/dev/sdj</p>\n<p>2. Reboot the system and verify the system has booted with the md device using the mount</p>\n<p>command.</p>\n<p>Add the old partition volume to the md raid-1 set:</p>\n<p>1. Change the partition of sdh1 and add the sdh1 device to md9 array and allow the resync</p>\n<p>to complete from sdh1 to sdj1.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdh 1 fd</p>\n<p>Done</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdh1</p>\n<p>mdadm: added /dev/sdh1</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# watch cat /proc/mdstat</p>\n<p>2. Reboot the system to verify the md migration completed successfully.</p>\n<p>3.</p>\n<p>Run the cat /proc /mdstat command to check the status of the running array.</p>\n<p>\u00a0</p>\n<p>Resizing an existing RAID Partition</p>\n<p>The Linux software-RAID solution allows us to resize (increase or decrease) the RAID partition size.</p>\n<p>Following steps explain how to increase the size of existing software RAID partition (data and OS</p>\n<p>partitions).</p>\n<p>Here is what is required:</p>\n<p>\u2022 Prepare partitions of the new size desired</p>\n<p>\u2022 Replace both RAID members with newly created partitions by breaking existing RAID</p>\n<p>\u00a0 \u2022</p>\n<p>\u00a0\u00a0\u00a0 \u2022</p>\n<p>\u00a0</p>\n<p>Resize the RAID array</p>\n<p>If dealing with OS partitions, Prepare the new RAID volume to be bootable</p>\n<p>Prepare partitions of the new size desired:</p>\n<p>1. Initially create the md9 raid-1 level with a size approximately 100 GB comprised of both the</p>\n<p>sdh1 and sdj1 volumes. The md-0 array RAID set will be increased to approximately 200 GB</p>\n<p>by using the sdl1 and sdm1 volumes.</p>\n<p>2. Create the new RAID partition of increased size on sdl and sdm. In this example, we created a</p>\n<p>new RAID partition of approximately 200GB in size on the sdl volume.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdl 1 fd</p>\n<p>Done</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# fdisk -l /dev/sdl</p>\n<p>Disk /dev/sdl: 1000.2 GB, 1000204886016 bytes</p>\n<p>255 heads, 63 sectors/track, 121601 cylinders</p>\n<p>Units = cylinders of 16065 * 512 = 8225280 bytes</p>\n<p>Sector size (logical/physical): 512 bytes / 512 bytes</p>\n<p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p>\n<p>Disk identifier: 0x4c0a9054</p>\n<p>Device Boot</p>\n<p>/dev/sdl1</p>\n<p>Start</p>\n<p>1</p>\n<p>25000</p>\n<p>End</p>\n<p>Blocks</p>\n<p>Id System</p>\n<p>200812468+ fd Linux raid autodetect</p>\n<p>\u00a0</p>\n<p>Replace both RAID members with newly created partitions by breaking existing RAID:</p>\n<p>1. Set the sdj1 volume to faulty and remove the volume from the RAID set.</p>\n<p>\u00a0</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm /dev/md9 /dev/sdj1</p>\n<p>mdadm: set /dev/sdj1 faulty in /dev/md9</p>\n<p>mdadm: hot removed /dev/sdj1 from /dev/md9</p>\n<p>2.</p>\n<p>\u2013-fail</p>\n<p>/dev/sdj1</p>\n<p>\u2013remove</p>\n<p>Add the new partition to the RAID set and allow the resync to complete on the new</p>\n<p>partition added to the RAID set. Run cat /proc/mdstat to show the status of</p>\n<p>resynchronization.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>\n<p>mdadm: added /dev/sdl1</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p>\n<p>Personalities : [RAID-1]</p>\n<p>md9 : active RAID-1 sdl1[2] sdh1[0]</p>\n<p>102399928 blocks super 1.0 [2/2] [UU]</p>\n<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>\n<p>unused devices: &lt;none&gt;</p>\n<p>3. Repeat the above steps adding the /dev/sdm1 partition. Remove the /dev/sdh1</p>\n<p>partition from the RAID set. Allow the resynchronization to complete on the /dev/sdm1</p>\n<p>partition.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p>\n<p>Personalities : [RAID-1]</p>\n<p>md9 : active RAID-1 sdm1[3] sdl1[2]</p>\n<p>102399928 blocks super 1.0 [2/1] [_U]</p>\n<p>[&gt;....................]</p>\n<p>recovery =</p>\n<p>finish=20.7min speed=81894K/sec</p>\n<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>\n<p>0.3%</p>\n<p>(409472/102399928)</p>\n<p>unused devices: &lt;none&gt;</p>\n<p>\u00a0</p>\n<p>Resize the RAID array:</p>\n<p>1. Set the /dev/md9 partition size to use the new volume partition size and allow the resync</p>\n<p>to complete.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p>\n<p>mdadm: Cann[ot set device size for /dev/md9: Device or resource busy</p>\n<p>Bitmap must be removed before size can be changed</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --bitmap none</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p>\n<p>mdadm: component size of /dev/md9 has been set to 200812396K</p>\n<p>2.</p>\n<p>Resize the file system of the /dev/md9 partition to increase the file system size. Now the</p>\n<p>df \u2013h command shows the increased md RAID set.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# resize2fs /dev/md9</p>\n<p>resize2fs 1.41.12 (17-May-2010)</p>\n<p>Filesystem at /dev/md9 is mounted on /; on-line resizing required</p>\n<p>old desc_blocks = 7, new_desc_blocks = 12</p>\n<p>Performing an on-line resize of /dev/md9 to 50203099 (4k) blocks.</p>\n<p>The filesystem on /dev/md9 is now 50203099 blocks long.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# df -h</p>\n<p>Filesystem</p>\n<p>Size Used Avail Use% Mounted on</p>\n<p>/dev/md9</p>\n<p>189G 1.4G 178G</p>\n<p>1% /</p>\n<p>tmpfs</p>\n<p>1.9G</p>\n<p>0 1.9G</p>\n<p>0% /dev/shm</p>\n<p>If dealing with OS partitions, prepare the new RAID volume to be bootable:</p>\n<p>Note: Following steps are required to boot the Linux system, if you are replacing the drive from</p>\n<p>RAID array which has boot loader and file system already installed.</p>\n<p>Before resizing the RAID partition, ensure that the md-raid set has two active drives, sdh1 and sdj1</p>\n<p>and that grub is installed in /dev/sdh. After the resizing operation, the md-raid set has been</p>\n<p>replaced with sdl1 and sdm1 volumes and boot loader has been installed on sdl volume.</p>\n<p>1. Add the sdl and sdm entries in device.map file.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p>\n<p># this device map was generated by anaconda</p>\n<p>(hd0)</p>\n<p>/dev/sdh</p>\n<p>(hd1)</p>\n<p>/dev/sdj</p>\n<p>(hd2)</p>\n<p>/dev/sdl</p>\n<p>(hd3)</p>\n<p>/dev/sdm</p>\n<p>2. Install the grub boot loader on the /dev/sdl volume and then remove the older drives</p>\n<p>from the system.</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdl</p>\n<p>Installation finished. No error reported.</p>\n<p>This is the contents of the device map /boot/grub/device.map.</p>\n<p>Check if this is correct or not. If any of the lines is incorrect,</p>\n<p>fix it and re-run the script `grub-install&#039;.</p>\n<p># this device map was generated by anaconda</p>\n<p>(hd0)</p>\n<p>/dev/sdh</p>\n<p>(hd1)</p>\n<p>/dev/sdj</p>\n<p>(hd2)</p>\n<p>/dev/sdl</p>\n<p>(hd3)</p>\n<p>/dev/sdm</p>\n<p>Reboot the system and enter the storage controller BIOS configuration.</p>\n<p>4. Change the boot drive to new drive where the boot loader is now installed.</p>\n<p>5. Save the configuration and restart the system to boot from new RAID set partition.</p>\n<p>3.</p>\n<p>\u00a0</p>\n<p>Recovery from a Broken RAID</p>\n<p>In case of RAID failures on a system running Linux Software RAID (md) solution (for example, media</p>\n<p>failure or disk driver failure), it is possible to recover the system by any of these methods</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>replacing the faulty disk</p>\n<p>adding a new disk</p>\n<p>using the spare disk</p>\n<p>In Linux software raid, recovery is achieved through \u201cfailover\u201d mechanisms. Failover mechanism</p>\n<p>ensures data protection by providing additional drives (spares) and can be automatic or manual.</p>\n<p>\u00a0</p>\n<p>Automatic Failover</p>\n<p>Linux md-raid solution has an intelligent monitor mechanism to detect hardware failure in RAID arrays.</p>\n<p>If any disk in the RAID array fails, the monitors sets the failed drive to faulty and starts using one of the</p>\n<p>available spare drives for regeneration. To check the status of the RAID array, look at /proc/mdstat</p>\n<p>file.</p>\n<p>To replicate a raid failure to check how automatic failover scenario works, follow the steps.</p>\n<p>1. Create a RAID-1 setup with three raid partition. Minimum two raid partitions are required</p>\n<p>to create RAID-1 device and third raid partition is used as a spare disk, will be used as a</p>\n<p>replacement if one of the active RAID partition fails.</p>\n<p>2. Simply pull out one of the disks which are active in the raid array and check the status of</p>\n<p>the array using the cat /proc/mdstat command, will show the removed drive as faulty</p>\n<p>and use the spare drive as replacement for date re-generation.</p>\n<p>\u00a0</p>\n<p>Manual Failover</p>\n<p>Faulty drives in the raid array can be replaced manually. Following steps discuss how to manually</p>\n<p>replace the faulty drive from the RAID-1 array:</p>\n<p>1. Raid-1 setup is created with sdh1 and sdj1 partitions. Set the sdh1 drive to faulty.</p>\n<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -f /dev/md9 /dev/sdh1</p>\n<p>mdadm: set /dev/sdh1 faulty in /dev/md9</p>\n<p>2. Remove the faulty drive sdh1 from the array.</p>\n<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -r /dev/md9 /dev/sdh1</p>\n<p>mdadm: hot removed /dev/sdh1 from /dev/md9</p>\n<p>3. Replace the faulty drive with adding a new one to the array.</p>\n<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>\n<p>Device Added</p>\n<p>4. Look at status of the RAID array by executing the cat /proc/mdstat command, showing</p>\n<p>/dev/sdl1 added to the RAID array. Also check that the resynchronization is complete.</p>\n<p>[root&#064;DELL-PowerEdge-R510 ~]# cat /proc/mdstat</p>\n<p>Personalities : [RAID-1]</p>\n<p>md9 : active RAID-1 sdl1[2] sdj1[0]</p>\n<p>40958908 blocks super 1.1 [2/2] [UU]</p>\n<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>\n<p>Adding a Spare Disk to the Array</p>\n<p>Spare disks provide additional protection to a raid configuration. If a disk fails in a raid array, the spare</p>\n<p>disk automatically replaces the failed drive; also the raid can be rebuilt automatically in the</p>\n<p>background. Spare drives can be added to the RAID array during the time of creation of the array or</p>\n<p>later.</p>\n<p>Adding spare disk during raid array creation</p>\n<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90 \u2013-spare-devices=1 /dev/sdm1</p>\n<p>mdadm: array /dev/md9 started.</p>\n<p>Adding spare disk to an existing array.</p>\n<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>\n<p>mdadm: added /dev/sdl1</p>\n<p>[root&#064;DELL-Poweredge-R510 ~]# cat /proc/mdstat</p>\n<p>Personalities : [RAID-1]</p>\n<p>md8 : active RAID-1 sdl1[2](S) sdh1[0] sdj1[1] sdl</p>\n<p>References</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>\u2022</p>\n<p>http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/index.html</p>\n<p>http://www.spinics.net/lists/raid/</p>\n<p>https://raid.wiki.kernel.org/index.php/Linux_Raid</p>\n<p>\u00a0</p>\n<p>-------------------------------------------------------------------------------------------------------------</p>\n<p>\u00a0</p>\n<p>\u00a0</p>\n<p>Zaustavljanje resync-a:</p>\n<p>echo &quot;idle&quot; &gt; /sys/block/md0/md/sync_action</p>\n<p>\u00a0</p>\n<p>Rename:</p>\n<p>\u00a0</p>\n<p>Last time I did this it was from a later distribution --- either a</p>\n<p>RHEL-4 or FC-5 rescue CD, I cannot recall which at this point.\u00a0 I did it</p>\n<p>precisely for the reason you give, to move an existing /dev/md0 to</p>\n<p>another RHEL-4 box.\u00a0 But it wasn&#039;t hard, using mdadm.\u00a0 Knowing the</p>\n<p>devices belonging to the array (sda5 and sdb5 in this case), I just did:</p>\n<p>\u00a0</p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # mdadm --stop /dev/md0</p>\n<p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 # mdadm -A /dev/md6 -m0 --update=super-minor /dev/sda5 /dev/sdb5</p>\n<p>\u00a0</p>\n<p>which stops the array as /dev/md0 and then reassembles it as /dev/md6.</p>\n<p>The reassembly looks for devices which have an existing minor number of</p>\n<p>0, not 6 (-m0), and then updates the minors in the superblocks to the</p>\n<p>new number.</p>\n<p>\u00a0</p>\n<p>I believe the same options are present in taroon&#039;s mdadm.</p>\n<p>\u00a0</p>\n<p>mdadm ---create --level=1 --raid-devices=2 /dev/sdh1 /dev/sdg1</p>\n<p>\u00a0</p>\n<p>(Repair filesystem) ne dozvoljava upis. resenje:</p>\n<p>mount -w -o remount /</p>\n<p>\u00a0</p>\n<p>Linux console scroll : Shift + PgDn ili Shift + PgUp</p>\n<p>\u00a0</p>\n<p>\u00a0</p>\n<p>Creating Boot RAID1:</p>\n<p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdd1 /dev/sde1 --metadata=0.90</p>\n<p>\u00a0</p>\n<p>Creating RAID10 far,2:</p>\n<p>\u00a0</p>\n<p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=10 --layout=f2 --raid-disks=2 /dev/sdd9 /dev/sde9</p>\n<p>\u00a0</p>\n<p>\u00a0</p>\n<p>6.3. Activate RAID Devices</p>\n<p>\u00a0</p>\n<p>The installation media doesn&#039;t include a default mdadm.conf file, so it must be created. An easy method is...</p>\n<p>\u00a0</p>\n<p>mdadm --examine --scan &gt; /etc/mdadm.conf</p>\n<p>\u00a0</p>\n<p>Next, activate all discovered RAID devices with...</p>\n<p>\u00a0</p>\n<p>mdadm --assemble --scan</p>\n<p>\u00a0</p>\n<p>\u00a0</p>\n<p>cat /proc/sys/dev/raid/speed_limit_max</p>\n<p>200000</p>\n<p>cat /proc/sys/dev/raid/speed_limit_min</p>\n<p>1000</p>\n<p>\u00a0</p>\n<p>50MB/s:</p>\n<p>echo 50000 &gt;/proc/sys/dev/raid/speed_limit_min</p>", "subject": "Software RAID on RHEL 6 ", "id": "10151254589767728", "icon": "https://fbstatic-a.akamaihd.net/rsrc.php/v2/yi/r/-64q65AWgXb.png"}