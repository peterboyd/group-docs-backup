<!DOCTYPE html>
<html>
    <head>
        <title>Software RAID on RHEL 6 </title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" >
    </head>
    <body>
        <b>TITLE OF THE DOCUMENT (just copy/paste it):</b>
        <div style="border: 2px solid; padding: 10px; margin: 10px 50px 50px;">
             Software RAID on RHEL 6 
        </div>

        <b>CONTENT OF THE DOCUMENT (just copy/paste it):</b>
        <div style="border: 2px solid; padding: 10px; margin: 10px 50px;">
            <p><strong>* NOTICE 1: Text is changed by Ljubomir Ljubojevic in some aspects.</strong></p>
<p><strong>* NOTICE 2: /dev/sd? and /dev/md? names are deliberetly changed to avoid accidental damage of existing partitions, and force you to edit them before you run given command.</strong></p>
<p><strong></strong></p>
<p><em><strong>Software RAID on RHEL 6</strong></em></p>
<p> </p>
<p>Installation, Migration and Recovery</p>
<p>November 2010</p>
<p>Ashokan Vellimalai</p>
<p>Raghavendra Biligiri</p>
<p>Dell │ Enterprise Operating Systems</p>
<p>THIS WHITE PAPER IS FOR INFORMATIONAL PURPOSES ONLY, AND MAY CONTAIN TYPOGRAPHICAL</p>
<p>ERRORS AND TECHNICAL INACCURACIES. THE CONTENT IS PROVIDED AS IS, WITHOUT EXPRESS OR</p>
<p>IMPLIED WARRANTIES OF ANY KIND.</p>
<p>© 2010 Dell Inc. All rights reserved. Reproduction of this material in any manner whatsoever without</p>
<p>the express written permission of Dell Inc. is strictly forbidden. For more information, contact Dell.</p>
<p>Dell, the DELL logo, and PowerEdge, are trademarks of Dell Inc. Red Hat Enterprise Linux® is a</p>
<p>registered trademark of Red Hat, Inc. in the United States and/or other countries. Other trademarks</p>
<p>and trade names may be used in this document to refer to either the entities claiming the marks and</p>
<p>names or their products. Dell Inc. disclaims any proprietary interest in trademarks and trade names</p>
<p>other than its own.</p>
<p>November 2010</p>
<p>Contents</p>
<p>Introduction ............................................................................... 4</p>
<p>Setting up Software RAID in RHEL 6 .......................................... 4</p>
<p>Setup during Installation ........................................................... 4</p>
<p>Setup after Installation .............................................................. 5</p>
<p>Migration of Storage from Non-RAID to RAID Configurations ..... 6</p>
<p>Resizing an existing RAID Partition ............................................. 8</p>
<p>Recovery from a Broken RAID .................................................. 11</p>
<p>Automatic Failover ................................................................... 11</p>
<p>Manual Failover ........................................................................ 11</p>
<p>Adding a Spare Disk to the Array ............................................. 12</p>
<p>References ............................................................................... 13</p>
<p> </p>
<p>Introduction</p>
<p>Software RAID is RAID that is implemented at the software layer without the need for a dedicated</p>
<p>hardware RAID controller on the system. Software RAID can be created on any storage block device</p>
<p>independent of storage controllers. On Linux based operating system (OS), software RAID functionality</p>
<p>is provided with the help of the md(4) (Multiple Device) driver and managed by the mdadm(8) utility.</p>
<p>“md” and “mdadm” in RHEL 6 support RAID levels 0, 1, 4, 5, 6, and 10.</p>
<p>Some notable advantages in using Software RAID over hardware RAID are:</p>
<p>• Software RAID is controller Independent which makes it a cost-effective solution.</p>
<p>• The RAID solution can easily be migrated to any storage block device.</p>
<p>• The entire software stack runs on a host CPU, with modern multi-core CPUs, this ensures</p>
<p>efficient CPU utilization.</p>
<p>• Software RAID provides a level of abstraction on underlying storage devices/technologies</p>
<p>This document attempts to provide step-by-step procedures that can be followed to:</p>
<p>• Set up software RAID in RHEL 6</p>
<p>• Migrate existing storage from Non-RAID to Software RAID</p>
<p>• Resize RAID volumes</p>
<p>• Recover from a broken RAID</p>
<p>This document uses RAID-1 as an example while working with Software RAID. The procedure can</p>
<p>however be applied to other RAID types as applicable. Please consult the mdadm(8) man page for</p>
<p>details on exact options for various RAID types.</p>
<p>We used a Dell PowerEdgeTM R510 server with a Dell PERC H200 storage controller on the system for this</p>
<p>procedure. All the storage volumes were exported directly to the OS without using any controller</p>
<p>hardware RAID features.</p>
<p>Note: Ensure that all data backed up before performing any of these procedures</p>
<p> </p>
<p>Setting up Software RAID in RHEL 6</p>
<p>Setup during Installation</p>
<p>The RHEL 6 installer (anaconda) has functionality that enables the OS to be installed on a software</p>
<p>RAID partition. This section describes the steps to install RHEL 6 on a RAID-1 partition.</p>
<p>1. Start the RHEL 6 installer and follow the on-screen installation instructions and select the</p>
<p>“Custom” layout for installation.</p>
<p>2. Create a minimum of two partitions to create RAID-1 device type and set the File System Type as</p>
<p>software raid.</p>
<p>3. Create a RAID-1 device from RAID members created in Step2, select the filesystem and RAID-1</p>
<p>level</p>
<p>Here is the minimum number of software RAID partitions required for each RAID level:</p>
<p>• RAID 0,1,10</p>
<p>– 2 partitions</p>
<p>• RAID 4,5</p>
<p>– 3 partitions</p>
<p>• RAID 6</p>
<p>– 4 partitions</p>
<p>4. After creating all the necessary partitions (/boot, /, swap, etc.) on RAID-1 volume, proceed with</p>
<p>the installation.</p>
<p>5. Once the installation is completed, the OS will boot successfully from the partitions on the RAID</p>
<p>volume.</p>
<p>Note: Ensure that the boot-loader is installed on the first disk and not on the RAID device.</p>
<p>Installing boot-loader on the RAID device may result in failure to boot the OS after installation.</p>
<p> </p>
<p>Setup after Installation</p>
<p>Software RAID volumes can be created on a running system post install as well. Ensure that the</p>
<p>partition/s on which the OS is installed are not used for creating software RAID partitions, failure to do</p>
<p>that may results in re-installing OS on the system.</p>
<p>Following section describes steps to create RAID-1 partition on the system.</p>
<p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1 and /dev/sdl1.</p>
<p>sdj1 and sdl1 are un-used partitions on this system</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90</p>
<p>mdadm: array /dev/md9 started.</p>
<p>2. Create ext4 filesystem layout on the md device and add array details mdadm –-detail</p>
<p>–scan to /etc/mdadm.conf file. Mount the device /dev/md9 on the system to use it.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 /data/</p>
<p>3. Add a new entry in /etc/fstab file to auto mount the md raid partition, whenever system</p>
<p>reboots.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p>
<p>/dev/md9</p>
<p>/data</p>
<p>ext3</p>
<p>defaults</p>
<p>1 1</p>
<p> </p>
<p>Migration of Storage from Non-RAID to RAID Configurations</p>
<p>It is possible to migrate to software raid, the “/” partition without having to re-install the operating</p>
<p>system if you installed RHEL 6 OS without software raid volumes. This section explains how migration of</p>
<p>storage from non-raid to raid configuration can be achieved.</p>
<p>At a high level, here is how it can be achieved:</p>
<p>•</p>
<p>•</p>
<p>•</p>
<p>•</p>
<p>•</p>
<p>Prepare the new storage volume</p>
<p>Update fstab and grub configuration to boot from newly created storage volume</p>
<p>Sync the data from the old partitions to the new storage volume</p>
<p>Install the boot loader on new storage volume</p>
<p>Add the old partition volume to the md raid-1 set</p>
<p>Prepare the new storage volume</p>
<p>1. Create the partition layout on the /dev/sdj volume similar to /dev/sdh.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -d /dev/sdh | sfdisk --force /dev/sdj</p>
<p>Device Boot</p>
<p>Start</p>
<p>End</p>
<p>#sectors Id System</p>
<p>/dev/sdj1</p>
<p>*</p>
<p>2048 20482047</p>
<p>20480000 83 Linux</p>
<p>/dev/sdj2</p>
<p>20482048 24578047</p>
<p>4096000 82 Linux swap / Solaris</p>
<p>Warning: partition 1 does not end at a cylinder boundary</p>
<p>Successfully wrote the new partition table</p>
<p>2. Set the partition id of /dev/sdj1 to Linux RAID</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdj 1 fd</p>
<p>Done</p>
<p> </p>
<p>Create RAID-1 using mdadm utility:</p>
<p>1. Create the raid-1 md device using the mdadm command with /dev/sdj1. Mark the first</p>
<p>volume as “missing”, which will be sdh volume, and it will be added later in the steps.</p>
<p>Since sdh has OS installed, we have to add this to raid array after copying the contents</p>
<p>from sdj to sdh drive.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 missing /dev/sdj1 --metadata=0.90</p>
<p>mdadm: array /dev/md9 started.</p>
<p>2. Create ext4 filesystem layout on the md device and add the array details mdadm –-</p>
<p>detail –scan to /etc/mdadm.conf file.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mkfs.ext4 /dev/md9</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --detail --scan &gt;&gt; /etc/mdadm.conf</p>
<p>Update fstab and grub configuration to boot from newly created storage volume:</p>
<p>1. Modify the /etc/fstab and /boot/grub/menu.lst with md device.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# blkid | grep –i md9</p>
<p>/dev/md9: UUID=&quot;016db049-6802-4369-bf66-bd48aad15395&quot; TYPE=&quot;ext4&quot;</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /etc/fstab</p>
<p>#UUID=38b56dff-c6d2-434f-bb48-25efb97f3a58 /</p>
<p>1 1</p>
<p>UUID=016db049-6802-4369-bf66-bd48aad15395 /</p>
<p>1 1</p>
<p>ext4 defaults</p>
<p>ext4 defaults</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/menu.lst</p>
<p>2. Add sdj to the device map entry to install the grub on the sdj device.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p>
<p># this device map was generated by anaconda</p>
<p>(hd0)</p>
<p>/dev/sdh</p>
<p>(hd1)</p>
<p>/dev/sdj</p>
<p>Sync the data from the old partitions to the new storage volume:</p>
<p>1. Since we are trying to replicate the contents from currently running partition. It is</p>
<p>recommended that you execute the following steps in run level1</p>
<p>2. Mount the array volume and copy the contents from sdh1 to md9.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mount /dev/md9 /mnt/</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# rsync -aqxP / /mnt/</p>
<p>Install the boot loader on new storage volume:</p>
<p>1. Install the boot loader on the sdj device.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdj</p>
<p>Installation finished. No error reported.</p>
<p>This is the contents of the device map /boot/grub/device.map.</p>
<p>Check if this is correct or not. If any of the lines is incorrect,</p>
<p>fix it and re-run the script `grub-install&#039;.</p>
<p># this device map was generated by anaconda</p>
<p>(hd0)</p>
<p>/dev/sdh</p>
<p>(hd1)</p>
<p>/dev/sdj</p>
<p>2. Reboot the system and verify the system has booted with the md device using the mount</p>
<p>command.</p>
<p>Add the old partition volume to the md raid-1 set:</p>
<p>1. Change the partition of sdh1 and add the sdh1 device to md9 array and allow the resync</p>
<p>to complete from sdh1 to sdj1.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdh 1 fd</p>
<p>Done</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdh1</p>
<p>mdadm: added /dev/sdh1</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# watch cat /proc/mdstat</p>
<p>2. Reboot the system to verify the md migration completed successfully.</p>
<p>3.</p>
<p>Run the cat /proc /mdstat command to check the status of the running array.</p>
<p> </p>
<p>Resizing an existing RAID Partition</p>
<p>The Linux software-RAID solution allows us to resize (increase or decrease) the RAID partition size.</p>
<p>Following steps explain how to increase the size of existing software RAID partition (data and OS</p>
<p>partitions).</p>
<p>Here is what is required:</p>
<p>• Prepare partitions of the new size desired</p>
<p>• Replace both RAID members with newly created partitions by breaking existing RAID</p>
<p>  •</p>
<p>    •</p>
<p> </p>
<p>Resize the RAID array</p>
<p>If dealing with OS partitions, Prepare the new RAID volume to be bootable</p>
<p>Prepare partitions of the new size desired:</p>
<p>1. Initially create the md9 raid-1 level with a size approximately 100 GB comprised of both the</p>
<p>sdh1 and sdj1 volumes. The md-0 array RAID set will be increased to approximately 200 GB</p>
<p>by using the sdl1 and sdm1 volumes.</p>
<p>2. Create the new RAID partition of increased size on sdl and sdm. In this example, we created a</p>
<p>new RAID partition of approximately 200GB in size on the sdl volume.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# sfdisk -c /dev/sdl 1 fd</p>
<p>Done</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# fdisk -l /dev/sdl</p>
<p>Disk /dev/sdl: 1000.2 GB, 1000204886016 bytes</p>
<p>255 heads, 63 sectors/track, 121601 cylinders</p>
<p>Units = cylinders of 16065 * 512 = 8225280 bytes</p>
<p>Sector size (logical/physical): 512 bytes / 512 bytes</p>
<p>I/O size (minimum/optimal): 512 bytes / 512 bytes</p>
<p>Disk identifier: 0x4c0a9054</p>
<p>Device Boot</p>
<p>/dev/sdl1</p>
<p>Start</p>
<p>1</p>
<p>25000</p>
<p>End</p>
<p>Blocks</p>
<p>Id System</p>
<p>200812468+ fd Linux raid autodetect</p>
<p> </p>
<p>Replace both RAID members with newly created partitions by breaking existing RAID:</p>
<p>1. Set the sdj1 volume to faulty and remove the volume from the RAID set.</p>
<p> </p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm /dev/md9 /dev/sdj1</p>
<p>mdadm: set /dev/sdj1 faulty in /dev/md9</p>
<p>mdadm: hot removed /dev/sdj1 from /dev/md9</p>
<p>2.</p>
<p>–-fail</p>
<p>/dev/sdj1</p>
<p>–remove</p>
<p>Add the new partition to the RAID set and allow the resync to complete on the new</p>
<p>partition added to the RAID set. Run cat /proc/mdstat to show the status of</p>
<p>resynchronization.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>
<p>mdadm: added /dev/sdl1</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p>
<p>Personalities : [RAID-1]</p>
<p>md9 : active RAID-1 sdl1[2] sdh1[0]</p>
<p>102399928 blocks super 1.0 [2/2] [UU]</p>
<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>
<p>unused devices: &lt;none&gt;</p>
<p>3. Repeat the above steps adding the /dev/sdm1 partition. Remove the /dev/sdh1</p>
<p>partition from the RAID set. Allow the resynchronization to complete on the /dev/sdm1</p>
<p>partition.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /proc/mdstat</p>
<p>Personalities : [RAID-1]</p>
<p>md9 : active RAID-1 sdm1[3] sdl1[2]</p>
<p>102399928 blocks super 1.0 [2/1] [_U]</p>
<p>[&gt;....................]</p>
<p>recovery =</p>
<p>finish=20.7min speed=81894K/sec</p>
<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>
<p>0.3%</p>
<p>(409472/102399928)</p>
<p>unused devices: &lt;none&gt;</p>
<p> </p>
<p>Resize the RAID array:</p>
<p>1. Set the /dev/md9 partition size to use the new volume partition size and allow the resync</p>
<p>to complete.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p>
<p>mdadm: Cann[ot set device size for /dev/md9: Device or resource busy</p>
<p>Bitmap must be removed before size can be changed</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --bitmap none</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --grow /dev/md9 --size=max</p>
<p>mdadm: component size of /dev/md9 has been set to 200812396K</p>
<p>2.</p>
<p>Resize the file system of the /dev/md9 partition to increase the file system size. Now the</p>
<p>df –h command shows the increased md RAID set.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# resize2fs /dev/md9</p>
<p>resize2fs 1.41.12 (17-May-2010)</p>
<p>Filesystem at /dev/md9 is mounted on /; on-line resizing required</p>
<p>old desc_blocks = 7, new_desc_blocks = 12</p>
<p>Performing an on-line resize of /dev/md9 to 50203099 (4k) blocks.</p>
<p>The filesystem on /dev/md9 is now 50203099 blocks long.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# df -h</p>
<p>Filesystem</p>
<p>Size Used Avail Use% Mounted on</p>
<p>/dev/md9</p>
<p>189G 1.4G 178G</p>
<p>1% /</p>
<p>tmpfs</p>
<p>1.9G</p>
<p>0 1.9G</p>
<p>0% /dev/shm</p>
<p>If dealing with OS partitions, prepare the new RAID volume to be bootable:</p>
<p>Note: Following steps are required to boot the Linux system, if you are replacing the drive from</p>
<p>RAID array which has boot loader and file system already installed.</p>
<p>Before resizing the RAID partition, ensure that the md-raid set has two active drives, sdh1 and sdj1</p>
<p>and that grub is installed in /dev/sdh. After the resizing operation, the md-raid set has been</p>
<p>replaced with sdl1 and sdm1 volumes and boot loader has been installed on sdl volume.</p>
<p>1. Add the sdl and sdm entries in device.map file.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# cat /boot/grub/device.map</p>
<p># this device map was generated by anaconda</p>
<p>(hd0)</p>
<p>/dev/sdh</p>
<p>(hd1)</p>
<p>/dev/sdj</p>
<p>(hd2)</p>
<p>/dev/sdl</p>
<p>(hd3)</p>
<p>/dev/sdm</p>
<p>2. Install the grub boot loader on the /dev/sdl volume and then remove the older drives</p>
<p>from the system.</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# grub-install /dev/sdl</p>
<p>Installation finished. No error reported.</p>
<p>This is the contents of the device map /boot/grub/device.map.</p>
<p>Check if this is correct or not. If any of the lines is incorrect,</p>
<p>fix it and re-run the script `grub-install&#039;.</p>
<p># this device map was generated by anaconda</p>
<p>(hd0)</p>
<p>/dev/sdh</p>
<p>(hd1)</p>
<p>/dev/sdj</p>
<p>(hd2)</p>
<p>/dev/sdl</p>
<p>(hd3)</p>
<p>/dev/sdm</p>
<p>Reboot the system and enter the storage controller BIOS configuration.</p>
<p>4. Change the boot drive to new drive where the boot loader is now installed.</p>
<p>5. Save the configuration and restart the system to boot from new RAID set partition.</p>
<p>3.</p>
<p> </p>
<p>Recovery from a Broken RAID</p>
<p>In case of RAID failures on a system running Linux Software RAID (md) solution (for example, media</p>
<p>failure or disk driver failure), it is possible to recover the system by any of these methods</p>
<p>•</p>
<p>•</p>
<p>•</p>
<p>replacing the faulty disk</p>
<p>adding a new disk</p>
<p>using the spare disk</p>
<p>In Linux software raid, recovery is achieved through “failover” mechanisms. Failover mechanism</p>
<p>ensures data protection by providing additional drives (spares) and can be automatic or manual.</p>
<p> </p>
<p>Automatic Failover</p>
<p>Linux md-raid solution has an intelligent monitor mechanism to detect hardware failure in RAID arrays.</p>
<p>If any disk in the RAID array fails, the monitors sets the failed drive to faulty and starts using one of the</p>
<p>available spare drives for regeneration. To check the status of the RAID array, look at /proc/mdstat</p>
<p>file.</p>
<p>To replicate a raid failure to check how automatic failover scenario works, follow the steps.</p>
<p>1. Create a RAID-1 setup with three raid partition. Minimum two raid partitions are required</p>
<p>to create RAID-1 device and third raid partition is used as a spare disk, will be used as a</p>
<p>replacement if one of the active RAID partition fails.</p>
<p>2. Simply pull out one of the disks which are active in the raid array and check the status of</p>
<p>the array using the cat /proc/mdstat command, will show the removed drive as faulty</p>
<p>and use the spare drive as replacement for date re-generation.</p>
<p> </p>
<p>Manual Failover</p>
<p>Faulty drives in the raid array can be replaced manually. Following steps discuss how to manually</p>
<p>replace the faulty drive from the RAID-1 array:</p>
<p>1. Raid-1 setup is created with sdh1 and sdj1 partitions. Set the sdh1 drive to faulty.</p>
<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -f /dev/md9 /dev/sdh1</p>
<p>mdadm: set /dev/sdh1 faulty in /dev/md9</p>
<p>2. Remove the faulty drive sdh1 from the array.</p>
<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm -r /dev/md9 /dev/sdh1</p>
<p>mdadm: hot removed /dev/sdh1 from /dev/md9</p>
<p>3. Replace the faulty drive with adding a new one to the array.</p>
<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>
<p>Device Added</p>
<p>4. Look at status of the RAID array by executing the cat /proc/mdstat command, showing</p>
<p>/dev/sdl1 added to the RAID array. Also check that the resynchronization is complete.</p>
<p>[root&#064;DELL-PowerEdge-R510 ~]# cat /proc/mdstat</p>
<p>Personalities : [RAID-1]</p>
<p>md9 : active RAID-1 sdl1[2] sdj1[0]</p>
<p>40958908 blocks super 1.1 [2/2] [UU]</p>
<p>bitmap: 1/1 pages [4KB], 65536KB chunk</p>
<p>Adding a Spare Disk to the Array</p>
<p>Spare disks provide additional protection to a raid configuration. If a disk fails in a raid array, the spare</p>
<p>disk automatically replaces the failed drive; also the raid can be rebuilt automatically in the</p>
<p>background. Spare drives can be added to the RAID array during the time of creation of the array or</p>
<p>later.</p>
<p>Adding spare disk during raid array creation</p>
<p>[root&#064;Dell-PowerEdge-R510 ~]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdj1 /dev/sdl1 --metadata=0.90 –-spare-devices=1 /dev/sdm1</p>
<p>mdadm: array /dev/md9 started.</p>
<p>Adding spare disk to an existing array.</p>
<p>[root&#064;DELL-PowerEdge-R510 ~]# mdadm --add /dev/md9 /dev/sdl1</p>
<p>mdadm: added /dev/sdl1</p>
<p>[root&#064;DELL-Poweredge-R510 ~]# cat /proc/mdstat</p>
<p>Personalities : [RAID-1]</p>
<p>md8 : active RAID-1 sdl1[2](S) sdh1[0] sdj1[1] sdl</p>
<p>References</p>
<p>•</p>
<p>•</p>
<p>•</p>
<p>http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/index.html</p>
<p>http://www.spinics.net/lists/raid/</p>
<p>https://raid.wiki.kernel.org/index.php/Linux_Raid</p>
<p> </p>
<p>-------------------------------------------------------------------------------------------------------------</p>
<p> </p>
<p> </p>
<p>Zaustavljanje resync-a:</p>
<p>echo &quot;idle&quot; &gt; /sys/block/md0/md/sync_action</p>
<p> </p>
<p>Rename:</p>
<p> </p>
<p>Last time I did this it was from a later distribution --- either a</p>
<p>RHEL-4 or FC-5 rescue CD, I cannot recall which at this point.  I did it</p>
<p>precisely for the reason you give, to move an existing /dev/md0 to</p>
<p>another RHEL-4 box.  But it wasn&#039;t hard, using mdadm.  Knowing the</p>
<p>devices belonging to the array (sda5 and sdb5 in this case), I just did:</p>
<p> </p>
<p>        # mdadm --stop /dev/md0</p>
<p>        # mdadm -A /dev/md6 -m0 --update=super-minor /dev/sda5 /dev/sdb5</p>
<p> </p>
<p>which stops the array as /dev/md0 and then reassembles it as /dev/md6.</p>
<p>The reassembly looks for devices which have an existing minor number of</p>
<p>0, not 6 (-m0), and then updates the minors in the superblocks to the</p>
<p>new number.</p>
<p> </p>
<p>I believe the same options are present in taroon&#039;s mdadm.</p>
<p> </p>
<p>mdadm ---create --level=1 --raid-devices=2 /dev/sdh1 /dev/sdg1</p>
<p> </p>
<p>(Repair filesystem) ne dozvoljava upis. resenje:</p>
<p>mount -w -o remount /</p>
<p> </p>
<p>Linux console scroll : Shift + PgDn ili Shift + PgUp</p>
<p> </p>
<p> </p>
<p>Creating Boot RAID1:</p>
<p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=1 --raid-disks=2 /dev/sdd1 /dev/sde1 --metadata=0.90</p>
<p> </p>
<p>Creating RAID10 far,2:</p>
<p> </p>
<p>[root&#064;kancelarija yum]# mdadm --create /dev/md9 --level=10 --layout=f2 --raid-disks=2 /dev/sdd9 /dev/sde9</p>
<p> </p>
<p> </p>
<p>6.3. Activate RAID Devices</p>
<p> </p>
<p>The installation media doesn&#039;t include a default mdadm.conf file, so it must be created. An easy method is...</p>
<p> </p>
<p>mdadm --examine --scan &gt; /etc/mdadm.conf</p>
<p> </p>
<p>Next, activate all discovered RAID devices with...</p>
<p> </p>
<p>mdadm --assemble --scan</p>
<p> </p>
<p> </p>
<p>cat /proc/sys/dev/raid/speed_limit_max</p>
<p>200000</p>
<p>cat /proc/sys/dev/raid/speed_limit_min</p>
<p>1000</p>
<p> </p>
<p>50MB/s:</p>
<p>echo 50000 &gt;/proc/sys/dev/raid/speed_limit_min</p>
        </div>
    </body>
</html>
